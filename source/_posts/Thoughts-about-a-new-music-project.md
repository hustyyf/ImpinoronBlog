---
title: A New Music Transcribe Project - Project WayToWest
date: 2020-02-25 20:59:57
tags: 
  - Sound processing
  - Ear Copy
  - Music Transcribe
  - Deep Learning
categories:
  - 思考与笔记 | Thoughts & Notes
cover: http://impinoron.cn/static/encoder-decoder.jpg
---
# Project - WayToWest
最近我在工作中接触到了挺多的音频处理算法，对音频内容的分离、提取有了一些最基本的理解。刚好最近又开始听起了摇滚，也因此想起了以前玩乐队那会儿的时光。那时候很多歌在网上没有免费的谱面资源，我们更多时候只能靠自己去听不同乐器的分配。所以我就开始想：有没有可能通过Deep Learning来进行一些扒谱的操作。于是我简单地构思了一个数据采集 --> 特征构建 --> 模型训练的过程。
目前已经打算正式的建立这个项目，开始在业余时间进行更多的尝试，并把相关的体验和想法记录在这个blog的“Project WayToWest”目录下面。登这个项目具备了雏形后我会把它挂到GitHub上。欢迎在现在开始阶段对这件事情感兴趣的朋友们联系我！

# 数据搜集

在搜集数据的时候为了尽可能简单的搜集数据，需要各种单一乐器在各种音调上的声音，强调**没有噪音**，同时要拥有尽可能高的采样率（不小于32khz）。不仅如此，每种乐器也要采集多种类的bpm。声音文件需要标记的内容有乐器，bpm以及自己的编号。

# 特征构造 & 模型训练

从模型的角度上讲，我借鉴了一些其它的算法，认为训练的方法主要是将不同的乐器音频混合在一起，再对合成好的音频进行操作。这里主要注意两个问题：扒谱音频处理的难点在于音乐中很多情况下，不同的乐器会在同一瞬间发出声音，这在声学特征上与随意的组合不同乐器有很大的差异。因此在一个训练sequence中，最好使用bpm相同或相近的音频组合进行训练，同时强调音频重音需要同步对齐。
对音频进行FFT得到频域信息。因为此后不需要语音分析所以raw frequency spectrum应该就够用了。截取一帧数据后分析这一帧数据的乐器组合。简单的靠RNN建立起一个基于连续帧数据判断的Multi-instrument Activity Detection。然后根据MIAD的结果以及中间层去获得不同乐器的GAIN或者频率信息。对合成声音进行频谱灰度图输出，对合成过程中的各个乐器进行单独的频谱灰度图输出，然后对对应位置灰度设计（0，1）的增益。可以采用的额外增益方法可以考虑使用注意力去着重选取图片局部信息。
这只是一个最初步的思考，在实际的项目过程中肯定还会遇到更多的问题。目前而言最头疼的七十九是数据搜集本身。最后，再次希望各位如果感兴趣的话可以和我一起讨论！